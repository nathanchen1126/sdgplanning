"""
================================================================================
éå‚æ•°æœºå™¨å­¦ä¹ ï¼šå…¨æŒ‡æ ‡æ± æ¥å…¥ + åŸå§‹/å¯¹æ•°å˜é‡å…±çº¿æ€§å¯¹æ¯” + åˆ†ç»„è´¡çŒ®åº¦å¯¼å‡º
æ•°æ®æ¥æºï¼šåŒ¹é…äº†ä¸°å¯Œè‡ªç„¶åœ°ç†ä¸æ”¿ç­–è¯•ç‚¹çš„æ–°æ•°æ®é›† (1ç»Ÿè®¡æ•°æ®åŒ¹é….xlsx)

ã€æœ€æ–°æ›´æ–°è¯´æ˜ã€‘ï¼š
    1. å…¨å˜é‡çº³å…¥ï¼šä¸æ‰‹åŠ¨å‰”é™¤ä»»ä½•å˜é‡ï¼Œ42ä¸ªç‰¹å¾å…¨æ™¯æ‰«æã€‚
    2. åŒè½¨åˆ¶å¯¹æ¯”ï¼šé’ˆå¯¹ç»å¯¹è§„æ¨¡å˜é‡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å€¼ä¸ ln(x+1) å¯¹æ•°åŒ–å€¼ï¼Œä»¥ä¾› VIF å¯¹æ¯”ã€‚
    3. åˆ†ç»„è§£é‡ŠåŠ›å¯¼å‡ºï¼šå°†ä¸‰å¤§ç»„åˆ«ï¼ˆåŸºç¡€ã€ç©ºé—´ã€æ”¿ç­–ï¼‰çš„æ€»è§£é‡ŠåŠ›åŠåŒ…å«å˜é‡å¯¼å‡ºè‡³ç‹¬ç«‹ Excelã€‚
================================================================================
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import shap
import matplotlib.pyplot as plt
import os

# é˜²æ­¢ä¸­æ–‡å­—ä½“æ˜¾ç¤ºæŠ¥é”™
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Microsoft YaHei']  
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# 1. åŸºç¡€è®¾ç½®ä¸æ•°æ®å‡†å¤‡ 
# ==========================================
input_file = r"D:\1sdgplanning\1data\1ç»Ÿè®¡æ•°æ®åŒ¹é….xlsx"
output_dir = r"D:\1sdgplanning\1data\å›å½’ç»“æœ"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

df = pd.read_excel(input_file, sheet_name='data')
y_col = 'total'

# --- å˜é‡åˆ†ç»„ä¸å¯¹æ•°åŒ–å¤„ç† ---
# æå–æ‰€æœ‰éœ€è¦å–å¯¹æ•°çš„ç»å¯¹è§„æ¨¡è¿ç»­å˜é‡
vars_to_log = [
    'population', 'education', 'hospital', 'library', 'property', 'gdp', 
    'expenditure', 'buildup', 'water', 'sci_expend', 'edu_expend', 'export', 'high_way'
]

# æ‰§è¡Œå¯¹æ•°åŒ–ï¼Œä½†ã€ä¸åˆ é™¤ã€‘åŸå˜é‡
for v in vars_to_log:
    if v in df.columns:
        df[f'ln_{v}'] = np.log(df[v] + 1)

# å°†å…¨é‡æŒ‡æ ‡ç³»ç»Ÿåœ°åˆ†é…åˆ°ä¸‰å¤§ç»´åº¦ä¸­ï¼ˆåŒ…å«åŸå§‹å€¼ä¸å¯¹æ•°å€¼ï¼‰
layer1_baseline = [
    'population', 'gdp',  'ln_buildup', 
]

layer2_spatial = [
    'ln_high_way',  'elevation_mean', 
    'slope_mean', 
     'ln_water', 'rain', 'yangtze_river', 'yellow_river', 'hu_line', 'coastal'
]

layer3_policy = [
    'ln_sci_expend', 'ln_edu_expend',
    'pro_capital', 'big_city', 'pilot_eco', 'pilot_fdi', 'pilot_ecosupervison', 
    'pilot_inno', 'pilot_urban', 'pilot_resilience', 'pilot_15min'
]

# åˆå¹¶æ‰€æœ‰ç‰¹å¾ï¼Œå¹¶ç¡®ä¿å®ƒä»¬åœ¨æ•°æ®é›†ä¸­ç¡®å®å­˜åœ¨
all_features_raw = layer1_baseline + layer2_spatial + layer3_policy
all_features = [f for f in all_features_raw if f in df.columns]

df_clean = df.dropna(subset=[y_col] + all_features).copy()

# æ ‡å‡†åŒ–å¤„ç†ï¼šä»…é’ˆå¯¹å…·æœ‰5ä¸ªä»¥ä¸Šå”¯ä¸€å€¼çš„è¿ç»­å˜é‡
continuous_features = [f for f in all_features if df_clean[f].nunique() > 5]
scaler = StandardScaler()
df_clean[continuous_features] = scaler.fit_transform(df_clean[continuous_features])

X = df_clean[all_features]
y = df_clean[y_col]

# ==========================================
# 2. å¤šé‡å…±çº¿æ€§è¯Šæ–­ (VIF)
# ==========================================
print("\n" + "="*50)
print(" æ­£åœ¨è®¡ç®—æ–¹å·®è†¨èƒ€å› å­ (VIF) è¯„ä¼°å¤šé‡å…±çº¿æ€§...")
print("="*50)

X_vif = sm.add_constant(X)
vif_data = pd.DataFrame()
vif_data["Variable"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]

vif_data = vif_data[vif_data["Variable"] != "const"].sort_values(by="VIF", ascending=False)
print(vif_data.head(10)) 

vif_output_path = os.path.join(output_dir, "VIF_Results_Full.xlsx")
vif_data.to_excel(vif_output_path, index=False)
print(f"-> å…¨å˜é‡ VIF ç»“æœå·²ä¿å­˜è‡³: {vif_output_path}")

# ==========================================
# 3. è®­ç»ƒæ¨¡å‹ä¸ç²¾åº¦è¯„ä¼°
# ==========================================
print("\n" + "="*50)
print(f" æœ‰æ•ˆæ ·æœ¬é‡: {len(df_clean)} | çº³å…¥ç‰¹å¾æ€»æ•°: {len(all_features)}")
print(" æ­£åœ¨è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
print("="*50)
rf_model = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_leaf=2, random_state=42, oob_score=True)
rf_model.fit(X, y)

y_pred = rf_model.predict(X)
r2 = r2_score(y, y_pred)
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
oob_r2 = rf_model.oob_score_

print(f"-> å†³å®šç³»æ•° R-squared (RÂ²):  {r2:.4f}")
print(f"-> å¹³å‡ç»å¯¹è¯¯å·® (MAE):        {mae:.4f}")
print(f"-> å‡æ–¹æ ¹è¯¯å·® (RMSE):       {rmse:.4f}")
print(f"-> OOB æ³›åŒ–ä¼°ç®— R-squared:  {oob_r2:.4f}")

# ==========================================
# 4. SHAP åˆ†æä¸å›¾è¡¨è¾“å‡º
# ==========================================
print("\næ­£åœ¨è®¡ç®— SHAP å€¼åˆ†æç‰¹å¾é‡è¦æ€§")
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X)

# å›¾1: SHAP æ€»ä½“ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾ (å±•ç¤ºå‰20ä¸ªæœ€é‡è¦å˜é‡)
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X, plot_type="bar", show=False, max_display=20)
plt.title("Drivers of SDG Heterogeneity (Top 20 Full Pool)", fontsize=14)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "SHAP_Importance_Bar_Full.png"), dpi=300)
plt.close()

# å›¾2: SHAP èœ‚æ‹¥åˆ†å¸ƒå›¾
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X, show=False, max_display=20)
plt.title("Directional Impact on SDGs (SHAP Summary Full)", fontsize=14)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "SHAP_Summary_Plot_Full.png"), dpi=300)
plt.close()

# ==========================================
# 5. æ ¸å¿ƒï¼šè®¡ç®—ç»´åº¦é©±åŠ¨åŠ›ã€åˆ†ç»„è§£é‡ŠåŠ›å¹¶å¯¼å‡º Excel
# ==========================================
shap_abs_mean = np.mean(np.abs(shap_values), axis=0)
importance_df = pd.DataFrame({'Variable': X.columns, 'Importance': shap_abs_mean})
total_impact = importance_df['Importance'].sum()

# æ„å»ºåˆ†ç»„å¯¼å‡ºåˆ—è¡¨
group_results = []
for group_name, group_vars in zip(
    ['1_åŸºç¡€å‘å±•æ§åˆ¶ (Baseline)', '2_ç©ºé—´ä¸è‡ªç„¶å±æ€§ (Spatial)', '3_æ”¿ç­–åå¥½ä¸å¹²é¢„ (Policy)'], 
    [layer1_baseline, layer2_spatial, layer3_policy]
):
    valid_vars = [v for v in group_vars if v in X.columns]
    group_imp = importance_df[importance_df['Variable'].isin(valid_vars)]['Importance'].sum()
    group_pct = (group_imp / total_impact) * 100
    
    group_results.append({
        'Group_Name': group_name,
        'Total_Importance': group_imp,
        'Contribution_Percentage(%)': group_pct,
        'Included_Variables': ", ".join(valid_vars)
    })

# --- æ‰“å°ç»„åˆ«ç»“è®º ---
print("\n" + "="*50)
print(" æ ¸å¿ƒç»“è®ºï¼šä¸‰å¤§ç»´åº¦çš„ç¡®åˆ‡é©±åŠ¨åŠ›å æ¯” ")
print("="*50)
for res in group_results:
    print(f"{res['Group_Name']}: {res['Contribution_Percentage(%)']:.2f}%")

# --- å¯¼å‡ºåˆ†ç»„ç»“æœ ---
group_df = pd.DataFrame(group_results)
group_output_path = os.path.join(output_dir, "Group_Importance_Results.xlsx")
group_df.to_excel(group_output_path, index=False)
print(f"\n-> åˆ†ç»„è§£é‡ŠåŠ›åŠåŒ…å«æŒ‡æ ‡æ¸…å•å·²ä¿å­˜è‡³: {group_output_path}")

# --- å¯¼å‡ºå¹¶æ‰“å°æ‰€æœ‰å•ä¸ªå˜é‡çš„è´¡çŒ®ç»“æœ ---
importance_df['Contribution_Percentage(%)'] = (importance_df['Importance'] / total_impact) * 100
importance_df = importance_df.sort_values(by='Importance', ascending=False)
importance_output_path = os.path.join(output_dir, "SHAP_Importance_Results_Full.xlsx")
importance_df.to_excel(importance_output_path, index=False)

print("\nğŸ† Top 10 å•ä¸€é©±åŠ¨å› å­ï¼š")
for idx, row in importance_df.head(10).iterrows():
    print(f" - {row['Variable']}: {row['Contribution_Percentage(%)']:.2f}%")
    
print(f"\nâœ… å…¨éƒ¨è·‘é€šå®Œæ¯•ï¼æ‰€æœ‰å…¨é‡åˆ†æç»“æœå‡å·²ä¿å­˜è‡³: {output_dir}")