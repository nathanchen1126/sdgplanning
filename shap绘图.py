"""
================================================================================
éå‚æ•°æœºå™¨å­¦ä¹ ä¸é«˜çº§SHAPå¯è§†åŒ–è„šæœ¬ (å·²ä¿®æ”¹é…è‰² & ä¿®å¤ç¼©è¿›)
================================================================================
å·¥ä½œå†…å®¹ï¼š
    1. ç»§æ‰¿åŸå§‹æ•°æ®å¤„ç†ã€éšæœºæ£®æ—è®­ç»ƒåŠVIFã€Group Importanceè®¡ç®—é€»è¾‘ã€‚
    2. æ•´åˆå‚è€ƒç»˜å›¾ä»£ç ï¼Œåˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ç»„åˆå¯è§†åŒ–å›¾è¡¨ï¼ˆç¯çŠ¶é¥¼å›¾+æ¡å½¢å›¾+èœ‚ç¾¤å›¾ï¼‰ã€‚
    3. å°†ç»“æœä¿å­˜è‡³æŒ‡å®šè·¯å¾„ D:\\1sdgplanning\\5fig
    4. å­—ä½“å¼ºåˆ¶Arialï¼Œå»é™¤å¤§æ ‡é¢˜ï¼Œä¿®æ”¹Xè½´èŒƒå›´ã€‚
    5. ç¼©å°ç¯çŠ¶å›¾å°ºå¯¸ï¼Œå¹¶å¼ºåˆ¶å°†æ ‡ç­¾é‡å‘½åä¸º Baseline, Spatial, Policyã€‚
    6. [ä¿®æ”¹]ï¼šèœ‚ç¾¤å›¾é…è‰²æ–¹æ¡ˆä»'coolwarm'ä¿®æ”¹ä¸ºå‚è€ƒçƒ­åŠ›å›¾çš„â€œæµ…ç»¿-æ·±æ£•â€é…è‰²æ–¹æ¡ˆã€‚

å·¥ç¨‹å¸ˆï¼šPythonä»£ç å·¥ç¨‹å¸ˆ
================================================================================
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import shap
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import matplotlib.colors as mcolors
from matplotlib.cm import ScalarMappable
import os
import warnings

# å¿½ç•¥shapè®¡ç®—æ—¶çš„ä¸€äº›ä¸å¿…è¦è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# ==========================================
# 0. å…¨å±€è®¾ç½® (Matplotlib & Paths)
# ==========================================
# å…¨å±€å­—ä½“å¼ºåˆ¶æ”¹ä¸º Arial
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial']
plt.rcParams['axes.unicode_minus'] = False
# è®¾ç½®å…¨å±€å­—ä½“å¤§å°ï¼Œä½¿å…¶æ›´æ¸…æ™°
plt.rcParams['font.size'] = 12

# è·¯å¾„è®¾ç½®
input_file = r"D:\1sdgplanning\1data\1ç»Ÿè®¡æ•°æ®åŒ¹é….xlsx"
# åŸå§‹å›å½’ç»“æœä¿ç•™ç›®å½•
output_dir_data = r"D:\1sdgplanning\1data\å›å½’ç»“æœ"
# ç”¨æˆ·æŒ‡å®šçš„æ–°å›¾åƒä¿å­˜ç›®å½•
output_dir_fig = r"D:\1sdgplanning\5fig"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for d in [output_dir_data, output_dir_fig]:
    if not os.path.exists(d):
        os.makedirs(d)

print(f"ç³»ç»Ÿå°±ç»ªã€‚æ•°æ®è¾“å…¥: {input_file}")
print(f"æ•°æ®ç»“æœè¾“å‡º: {output_dir_data}")
print(f"å›¾åƒç»“æœè¾“å‡º: {output_dir_fig}")

# ==========================================
# 1. æ•°æ®å‡†å¤‡
# ==========================================
print("\n" + "="*30)
print(" æ­¥éª¤ 1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
print("="*30)

df = pd.read_excel(input_file, sheet_name='data')
y_col = 'total'

# --- å˜é‡åˆ†ç»„ä¸å¯¹æ•°åŒ–å¤„ç† ---
vars_to_log = [
    'population', 'education', 'hospital', 'library', 'property', 'gdp', 
    'expenditure', 'buildup', 'water', 'sci_expend', 'edu_expend', 'export', 'high_way'
]

# æ‰§è¡Œå¯¹æ•°åŒ–ï¼Œä½†ã€ä¸åˆ é™¤ã€‘åŸå˜é‡
for v in vars_to_log:
    if v in df.columns:
        df[f'ln_{v}'] = np.log(df[v] + 1)

# å°†å…¨é‡æŒ‡æ ‡ç³»ç»Ÿåœ°åˆ†é…åˆ°ä¸‰å¤§ç»´åº¦ä¸­
layer1_baseline = ['population', 'gdp',  'ln_buildup']
layer2_spatial = ['ln_high_way', 'elevation_mean', 'slope_mean', 'ln_water', 'rain', 'yangtze_river', 'yellow_river', 'hu_line', 'coastal']
layer3_policy = ['ln_sci_expend', 'ln_edu_expend', 'pro_capital', 'big_city', 'pilot_eco', 'pilot_fdi', 'pilot_ecosupervison', 'pilot_inno', 'pilot_urban', 'pilot_resilience', 'pilot_15min']

# åˆ›å»ºç‰¹å¾åç§°åˆ°ç»„åçš„æ˜ å°„å­—å…¸ï¼Œç”¨äºç»˜å›¾é…è‰²
feature_group_map = {}
for feat in layer1_baseline: feature_group_map[feat] = 'Baseline'
for feat in layer2_spatial: feature_group_map[feat] = 'Spatial'
for feat in layer3_policy: feature_group_map[feat] = 'Policy'

# åˆå¹¶æ‰€æœ‰ç‰¹å¾ï¼Œå¹¶ç¡®ä¿å®ƒä»¬åœ¨æ•°æ®é›†ä¸­ç¡®å®å­˜åœ¨
all_features_raw = layer1_baseline + layer2_spatial + layer3_policy
all_features = [f for f in all_features_raw if f in df.columns]

df_clean = df.dropna(subset=[y_col] + all_features).copy()

# æ ‡å‡†åŒ–å¤„ç†ï¼šä»…é’ˆå¯¹å…·æœ‰5ä¸ªä»¥ä¸Šå”¯ä¸€å€¼çš„è¿ç»­å˜é‡
continuous_features = [f for f in all_features if df_clean[f].nunique() > 5]
scaler = StandardScaler()
df_clean[continuous_features] = scaler.fit_transform(df_clean[continuous_features])

X = df_clean[all_features]
y = df_clean[y_col]

# ==========================================
# 2. å¤šé‡å…±çº¿æ€§è¯Šæ–­ (VIF)
# ==========================================
print("\n" + "="*30)
print(" æ­¥éª¤ 2: å¤šé‡å…±çº¿æ€§è¯Šæ–­ (VIF)")
print("="*30)

X_vif = sm.add_constant(X)
vif_data = pd.DataFrame()
vif_data["Variable"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]

vif_data = vif_data[vif_data["Variable"] != "const"].sort_values(by="VIF", ascending=False)
vif_output_path = os.path.join(output_dir_data, "VIF_Results_Full.xlsx")
vif_data.to_excel(vif_output_path, index=False)
print(f"-> VIF ç»“æœ (Top 5):\n{vif_data.head(5)}")
print(f"-> å…¨å˜é‡ VIF ç»“æœå·²ä¿å­˜è‡³: {vif_output_path}")

# ==========================================
# 3. è®­ç»ƒæ¨¡å‹ä¸ç²¾åº¦è¯„ä¼°
# ==========================================
print("\n" + "="*30)
print(" æ­¥éª¤ 3: è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹")
print("="*30)
print(f" æœ‰æ•ˆæ ·æœ¬é‡: {len(df_clean)} | çº³å…¥ç‰¹å¾æ€»æ•°: {len(all_features)}")

# å‚è€ƒç»˜å›¾ä»£ç å»ºè®®ï¼Œæ·»åŠ  n_jobs=-1 åŠ é€Ÿè®¡ç®—
rf_model = RandomForestRegressor(n_estimators=1000, max_depth=15, min_samples_leaf=2, random_state=42, oob_score=True, n_jobs=-1)
rf_model.fit(X, y)

y_pred = rf_model.predict(X)
oob_r2 = rf_model.oob_score_
print(f"-> æ¨¡å‹è®­ç»ƒå®Œæˆã€‚OOB RÂ²: {oob_r2:.4f}, è®­ç»ƒé›† RÂ²: {r2_score(y, y_pred):.4f}")

# ==========================================
# 4. è®¡ç®— SHAP å€¼
# ==========================================
print("\n" + "="*30)
print(" æ­¥éª¤ 4: è®¡ç®— SHAP å€¼")
print("="*30)
explainer = shap.TreeExplainer(rf_model)
# æ³¨æ„ï¼šç›´æ¥ä¼ å…¥æ ‡å‡†åŒ–åçš„X
shap_values_array = explainer.shap_values(X)
# åˆ›å»º Explanation å¯¹è±¡ï¼Œç”¨äº summary_plot
shap_explanation = explainer(X)

# ==========================================
# 5. è®¡ç®—ç»´åº¦è´¡çŒ®åº¦
# ==========================================
print("\n" + "="*30)
print(" æ­¥éª¤ 5: è®¡ç®—ç›¸å¯¹é‡è¦æ€§ä¸ç»´åº¦è´¡çŒ®")
print("="*30)

# è®¡ç®—å•ä¸ªå˜é‡çš„ Mean(|SHAP|)
shap_abs_mean = np.mean(np.abs(shap_values_array), axis=0)
importance_df = pd.DataFrame({'Variable': X.columns, 'Importance': shap_abs_mean})
total_global_impact = importance_df['Importance'].sum()

# æ„å»ºåˆ†ç»„å¯¼å‡ºåˆ—è¡¨
group_results = []
groups_data = [
    ('1_Baseline Development', layer1_baseline), 
    ('2_Spatial & Natural', layer2_spatial), 
    ('3_Policy & Intervention', layer3_policy)
]

for group_name, group_vars in groups_data:
    valid_vars = [v for v in group_vars if v in X.columns]
    group_imp = importance_df[importance_df['Variable'].isin(valid_vars)]['Importance'].sum()
    group_pct = (group_imp / total_global_impact) * 100
    
    group_results.append({
        'Group_Name': group_name,
        'Total_Importance': group_imp,
        'Contribution_Percentage(%)': group_pct,
        'Variables': valid_vars # ä¿ç•™åˆ—è¡¨ä¾›ç»˜å›¾ä½¿ç”¨
    })

# --- æ‰“å°å¹¶å¯¼å‡ºç»„åˆ«ç»“è®º ---
print(" ğŸ† ä¸‰å¤§ç»´åº¦é©±åŠ¨åŠ›å æ¯”:")
for res in group_results:
    print(f" - {res['Group_Name']}: {res['Contribution_Percentage(%)']:.2f}%")

group_df = pd.DataFrame(group_results)
group_df_export = group_df.copy()
group_df_export['Included_Variables'] = group_df_export['Variables'].apply(lambda x: ", ".join(x))
group_output_path = os.path.join(output_dir_data, "Group_Importance_Results.xlsx")
group_df_export.drop(columns=['Variables']).to_excel(group_output_path, index=False)

# --- å¯¼å‡ºæ‰€æœ‰å•ä¸ªå˜é‡çš„è´¡çŒ®ç»“æœ ---
importance_df['Contribution_Percentage(%)'] = (importance_df['Importance'] / total_global_impact) * 100
importance_df = importance_df.sort_values(by='Importance', ascending=False)
importance_output_path = os.path.join(output_dir_data, "SHAP_Importance_Results_Full.xlsx")
importance_df.to_excel(importance_output_path, index=False)


# ==========================================
# 6. é«˜çº§ç»„åˆç»˜å›¾ 
# ==========================================
print("\n" + "="*30)
print(" æ­¥éª¤ 6: ç»˜åˆ¶ SHAP ç»„åˆå›¾")
print("="*30)

CMAP_BASE = plt.cm.get_cmap('coolwarm')

# åŸæ¥çš„è®¾ç½®ä¿æŒä¸å˜
MAX_DISPLAY = 15 # å›¾ä¸­å±•ç¤ºå‰15ä¸ªå˜é‡ï¼Œé¿å…æ‹¥æŒ¤

# å®šä¹‰ç»„åˆ«çš„å›ºå®šé…è‰² (ç”¨äºæ¡å½¢å›¾å’Œé¥¼å›¾)
group_colors_map = {
    'Baseline': '#8b6c42', # è«å…°è¿ªæ£•
    'Spatial': '#59a14f',  # è«å…°è¿ªç»¿
    'Policy': '#edc948'   # è«å…°è¿ªé»„
}

# æ ‡ç­¾æ˜ å°„å­—å…¸ (å¼ºåˆ¶é‡å‘½åé¥¼å›¾æ ‡ç­¾)
pie_label_mapping = {
    '1_Baseline Development': 'Baseline',
    '2_Spatial & Natural': 'Spatial',
    '3_Policy & Intervention': 'Policy'
}

def plot_shap_combined(X_df, shap_values, explanation, importance_df, group_results, feature_group_map, save_path):
    """
    åˆ›å»ºä¸€ä¸ªå¤æ‚çš„è‡ªå®šä¹‰ç»„åˆå›¾ï¼šå·¦ä¾§åµŒå…¥ç¯çŠ¶é¥¼å›¾çš„æ¡å½¢å›¾ï¼Œå³ä¾§å¯¹é½çš„èœ‚ç¾¤å›¾ã€‚
    """
    # 1. æ•°æ®å‡†å¤‡ 
    top_df = importance_df.head(MAX_DISPLAY).copy()
    sorted_features = top_df['Variable'].tolist()
    sorted_idx = [X_df.columns.get_loc(f) for f in sorted_features]
    
    # å‡†å¤‡æ¡å½¢å›¾é¢œè‰² 
    bar_colors = [group_colors_map[feature_group_map[f]] for f in sorted_features]
    
    # 2. åˆ›å»ºç”»å¸ƒ
    fig = plt.figure(figsize=(22, 10)) 

    # å…¨å±€åæ ‡å‚æ•° 
    plot_bottom = 0.1
    plot_height = 0.8
    space_between = 0.04
    
    # 3. è®¡ç®—åæ ‡è½´ä½ç½® 
    # --- A. ä¸­å¤®æ¡å½¢å›¾ ---
    bar_width = 0.25
    bar_left = 0.38 
    ax_bar = fig.add_axes([bar_left, plot_bottom, bar_width, plot_height])
    
    # --- B. åµŒå…¥å¼ç¯å½¢é¥¼å›¾ (Donut pie) ---
    pie_size = 0.25
    pie_left = bar_left + 0.04 # å¾®è°ƒå·¦è¾¹è·é€‚é…ç¼©å°åçš„å°ºå¯¸
    pie_bottom = plot_bottom + 0.04 # å¾®è°ƒåº•è¾¹è·
    
    # ç›´æ¥ä½¿ç”¨æ™®é€šåæ ‡ç³»ï¼ˆå»æ‰polarï¼‰ï¼Œå¹¶è®¾ç½®èƒŒæ™¯é€æ˜
    ax_pie = fig.add_axes([pie_left, pie_bottom, pie_size, pie_size])
    ax_pie.patch.set_alpha(0.0)

    # --- C. å³ä¾§èœ‚çªå›¾ ---
    beeswarm_left = bar_left + bar_width + space_between
    beeswarm_width = 0.32
    ax_beeswarm = fig.add_axes([beeswarm_left, plot_bottom, beeswarm_width, plot_height])

    # --- D. Colorbar ---
    cbar_width = 0.01
    cbar_left = beeswarm_left + beeswarm_width + 0.01
    ax_cbar = fig.add_axes([cbar_left, plot_bottom + plot_height*0.2, cbar_width, plot_height*0.6])

    # ---ç»˜å›¾å¼€å§‹---

    print(" -> ç»˜åˆ¶ä¸­å¤®æ¡å½¢å›¾ä¸ç»´åº¦é…è‰²...")
    # --- A. ä¸­å¤®æ¡å½¢å›¾ (ax_bar) ---
    y_pos = np.arange(len(sorted_features))
    ax_bar.barh(y_pos, top_df['Importance'], color=bar_colors, height=0.5, edgecolor='none', alpha=0.9)
    
    ax_bar.set_yticks(y_pos)
    ax_bar.set_yticklabels(sorted_features, fontsize=16)
    ax_bar.invert_yaxis() 
    ax_bar.set_xlabel('Mean(|SHAP Value|)', fontsize=18, labelpad=10)
    
    ax_bar.set_xlim(0, 0.015)
    
    ax_bar.spines['left'].set_visible(False)
    ax_bar.spines['top'].set_visible(False)
    ax_bar.spines['right'].set_linewidth(2)
    ax_bar.spines['bottom'].set_linewidth(2)
    ax_bar.tick_params(axis='x', labelsize=16, direction='in', length=6, width=2)
    ax_bar.tick_params(axis='y', length=0) 
    ax_bar.grid(axis='x', linestyle='--', alpha=0.5)

    print(" -> ç»˜åˆ¶é€æ˜ç»´åº¦ç¯çŠ¶é¥¼å›¾...")
    # --- B. åµŒå…¥å¼ç¯çŠ¶é¥¼å›¾ (ax_pie) ---
    percentages = [res['Contribution_Percentage(%)'] for res in group_results]
    
    # ä½¿ç”¨æ˜ å°„å­—å…¸ä¸¥æ ¼é‡å‘½åæ ‡ç­¾
    group_labels_clean = [pie_label_mapping[res['Group_Name']] for res in group_results]
    
    radial_inner_colors = [group_colors_map[pie_label_mapping[res['Group_Name']]] for res in group_results]

    # wedgeprops é‡Œé¢çš„ width=0.4 å®ç°äº†ä¸­é—´ç•™ç©ºçš„â€œç¯å½¢â€æ•ˆæœ
    wedges, texts, autotexts = ax_pie.pie(
        percentages, 
        labels=group_labels_clean,
        colors=radial_inner_colors,
        autopct='%1.1f%%',
        startangle=90,           # ä»æœ€ä¸Šæ–¹12ç‚¹é’Ÿæ–¹å‘å¼€å§‹ç”»
        counterclock=False,      # é¡ºæ—¶é’ˆæ–¹å‘ç»˜åˆ¶
        wedgeprops=dict(width=0.4, edgecolor='white', linewidth=2, alpha=0.8),
        textprops=dict(color='black')
    )
    
    # ç¾åŒ–ç¯çŠ¶å›¾ä¸Šçš„æ–‡å­—å­—ä½“
    for text in texts:
        text.set_fontsize(13)
    for autotext in autotexts:
        autotext.set_fontsize(13)
        

    print(" -> ç»˜åˆ¶å³ä¾§SHAPèœ‚ç¾¤å›¾...")
    # --- C. å³ä¾§èœ‚çªå›¾ (ax_beeswarm) ---
    plt.sca(ax_beeswarm) 
    
    shap_values_sorted = shap_values[:, sorted_idx]
    X_data_sorted = X_df.iloc[:, sorted_idx]
    
    # ä½¿ç”¨è‡ªå®šä¹‰çš„ CMAP_BASEï¼ˆç»§æ‰¿è‡ªå‚è€ƒçƒ­åŠ›å›¾ï¼‰
    shap.summary_plot(
        shap_values_sorted, 
        X_data_sorted, 
        plot_type="dot", 
        cmap=CMAP_BASE,     # å·²åº”ç”¨è‡ªå®šä¹‰ CMAP
        max_display=MAX_DISPLAY, 
        show=False, 
        plot_size=None,     
        color_bar=False     
    )
    
    ax_beeswarm.set_yticklabels([]) 
    ax_beeswarm.set_ylabel('')
    ax_beeswarm.set_xlabel("SHAP Value", fontsize=18, labelpad=10)
    ax_beeswarm.invert_yaxis() 
    
    ax_beeswarm.spines['left'].set_visible(False)
    ax_beeswarm.spines['top'].set_visible(False)
    ax_beeswarm.spines['right'].set_visible(False)
    ax_beeswarm.spines['bottom'].set_linewidth(2)
    ax_beeswarm.tick_params(axis='x', labelsize=16, direction='in', length=6, width=2)
    ax_beeswarm.set_xlim(ax_beeswarm.get_xlim()) 

    # --- D. æ‰‹åŠ¨æ·»åŠ  Colorbar ---
    # æ›´æ–° ScalarMappable çš„ cmap
    m = ScalarMappable(cmap=CMAP_BASE)
    m.set_array([0, 1]) 
    cb = fig.colorbar(m, cax=ax_cbar, ticks=[0, 1])
    cb.set_label('Feature Value', size=16, labelpad=-10)
    cb.ax.set_yticklabels(['Low', 'High'], fontsize=14)
    cb.outline.set_visible(False) 

    # ä¿å­˜å›¾å½¢
    print(f" -> æ­£åœ¨ä¿å­˜ç»„åˆå›¾è‡³: {save_path} ...")
    plt.savefig(save_path, dpi=300, bbox_inches='tight', transparent=False)
    plt.close(fig)

# æ‰§è¡Œç»˜å›¾
final_fig_path = os.path.join(output_dir_fig, "shapç»„åˆå›¾_harmonized.jpg") # ä¿®æ”¹æ–‡ä»¶åä»¥åŒºåˆ†
plot_shap_combined(X, shap_values_array, shap_explanation, importance_df, group_results, feature_group_map, final_fig_path)

print("\nğŸ† å·¥ä½œå®Œæˆï¼")
print(f"âœ… ç›¸å¯¹é‡è¦æ€§æ•°æ®ç»“æœä¿å­˜åœ¨: {output_dir_data}")
print(f"âœ… é«˜çº§SHAPç»„åˆå›¾ä¿å­˜åœ¨: {final_fig_path}")